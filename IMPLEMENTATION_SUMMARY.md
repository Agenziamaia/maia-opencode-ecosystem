# Research Auto-Fallback - Implementation Summary

## âœ… Implementation Complete

**Status**: Deployed and ready for use.
**Approach**: Prompt-level retry logic (no plugins, no scripts required).
**User Impact**: Zero - completely transparent.

---

## What Was Implemented

### 1. MAIA System Prompt Update

**File**: `.opencode/agents/maia.md`

Added a new section "ðŸ”„ Research Failure Fallback (AUTOMATIC - Transparent to User)" that:

- **Defines failure indicators**: List of error patterns MAIA should detect (rate limits, overload, model not found, timeout, service unavailable, API key errors, resource exhausted)
- **Specifies fallback protocol**:
  1. Output exactly one line: `"Gemini is overloaded; switching to fast model and continuing."`
  2. Immediately retry with `@researcher_fast`
  3. Continue normal workflow
  4. No user commands required
  5. No additional context beyond the single-line notification
- **Provides clear example**: Shows the exact conversation flow when fallback triggers
- **Emphasizes automation**: Explicitly states "FULLY AUTOMATIC" and "NO commands, NO manual intervention required"

### 2. User Documentation

**File**: `RESEARCH_AUTOFALLBACK.md`

Explains the feature to users in simple terms:

- "Just ask normally" - the core message
- How it works (transparent fallback)
- When it triggers (list of failure conditions)
- What users see (single-line notification)
- Quality assurance (both models follow same standards)
- Option to force fast mode (but default is automatic)

### 3. Validation Guide

**File**: `RESEARCH_AUTOFALLBACK_VALIDATION.md`

Comprehensive testing and verification guide:

- Three test scenarios (simulated failure, normal operation, direct fast usage)
- Verification checklist (system prompt, config, documentation)
- Real-world validation approach
- Acceptance criteria
- Troubleshooting guide

---

## How It Works

### Before This Implementation

```
User: "Research React Server Components"
MAIA: "Oracle @researcher, research RSC..."
[Researcher fails with rate limit]
User: ðŸ˜  "Now I have to run /research-fast command manually"
```

### After This Implementation

```
User: "Research React Server Components"
MAIA: "Oracle @researcher, research RSC..."
[Researcher fails with rate limit]
MAIA: "Gemini is overloaded; switching to fast model and continuing."
MAIA: "Oracle @researcher_fast, research RSC..."
[Researcher_fast succeeds]
MAIA: [Continues normally with results]
User: ðŸ˜Š "That was seamless"
```

---

## Technical Details

### Why Prompt-Level Solution?

1. **Stays within OpenCode capabilities**: No plugins, no scripts, no modifications to OpenCode core
2. **Minimal changes**: Only updated MAIA's agent prompt and added documentation
3. **Maintainable**: Clear protocol in one place (MAIA's system prompt)
4. **Transparent**: Works with OpenCode's existing agent delegation system
5. **No dependencies**: Doesn't require Node.js scripts, bash wrappers, or external tools

### Why Not a Plugin/Script?

A plugin would:

- Require Node.js code maintenance
- Introduce complexity and potential bugs
- Need updates if OpenCode API changes
- Be harder to understand and debug
- Add another failure point

The prompt-level solution:

- Leverages GPT-5.2's intelligence to detect and handle failures
- Is self-documenting (the protocol is in the prompt)
- Works immediately without additional infrastructure
- Can be easily modified by editing a text file
- Fits perfectly within OpenCode's design philosophy

---

## Files Modified/Created

### Modified

- `.opencode/agents/maia.md` - Added fallback protocol section

### Created

- `RESEARCH_AUTOFALLBACK.md` - User-facing documentation
- `RESEARCH_AUTOFALLBACK_VALIDATION.md` - Validation and testing guide

### No Changes Required

- `opencode.json` - Already has both `researcher` and `researcher_fast` configured
- `.opencode/agents/researcher.md` - Already configured
- `.opencode/agents/researcher_fast.md` - Already configured

---

## Verification

### Quick Verification

1. **Check MAIA's prompt**:

   ```bash
   grep -A 30 "Research Failure Fallback" .opencode/agents/maia.md
   ```

   Should show the complete fallback protocol.

2. **Check documentation exists**:

   ```bash
   ls -la RESEARCH_AUTOFALLBACK*.md
   ```

   Should show two files.

3. **Verify both agents configured**:
   ```bash
   grep -A 5 '"researcher"' opencode.json
   grep -A 5 '"researcher_fast"' opencode.json
   ```
   Should show both agents with correct models.

### Functional Verification

See `RESEARCH_AUTOFALLBACK_VALIDATION.md` for detailed testing procedures.

---

## Usage

### For Users

**Nothing changes.** Just ask MAIA for research normally. If the deep researcher fails, MAIA automatically switches to the fast researcher and you'll see a single line: `"Gemini is overloaded; switching to fast model and continuing."`

### For Developers/Operators

Review the implementation in `.opencode/agents/maia.md` to understand the protocol. The fallback logic is fully contained in MAIA's system prompt and can be adjusted by editing that file.

---

## Future Enhancements (Optional)

If needed, the system could be enhanced by:

1. **Retry count**: Add a limit to prevent infinite retry loops (e.g., max 1 retry)
2. **Fallback chain**: Add more fallback options (e.g., a local LLM)
3. **Telemetry**: Log fallback events for analytics
4. **Custom messages**: Allow user-configurable notification messages

However, the current implementation is sufficient for the stated requirements:

- âœ… Zero manual commands
- âœ… Zero model switching by user
- âœ… Transparent handling
- âœ… Minimal changes
- âœ… No external dependencies

---

## Summary

The research auto-fallback system is now fully operational. It:

- âœ… Automatically handles researcher failures
- âœ… Transitions seamlessly to fast researcher
- âœ… Shows only a single-line notification to users
- âœ… Requires no manual commands or intervention
- âœ… Stays within OpenCode's capabilities
- âœ… Uses only prompt-level configuration
- âœ… Is fully documented for users and validators

**Users just ask normally. MAIA handles the rest.**

---

_Implementation Date: 2026-01-21_
_Implementer: @coder (GLM-4.7)_
_Approach: Prompt-level retry logic_
_Status: âœ… Complete_
